# Subtitle Flashcards

Subtitle Flashcards helps you turn TOEFL/GRE-level vocabulary hidden in English subtitles into lightweight study decks. Upload any `.srt` file and the app parses it locally, matches against a curated difficult-word list, and surfaces the highest-frequency words so you can review them in context.

## Highlights

- ğŸš€ **Local parsing** â€“ all subtitle processing happens in the browser, so your file never leaves the device.
- ğŸ“š **Curated word bank** â€“ ships with a TOEFL/GRE difficult-word list and only counts the words that appear in it.
- ğŸ“ˆ **Stats at a glance** â€“ shows total subtitle lines, unique difficult words, and aggregate occurrences.
- ğŸ§¾ **Top-word list** â€“ ranks the 50 most frequent difficult words to accelerate manual flashcard creation.
- ğŸ’¬ **Context + translation** â€“ each word comes with CC-CEDICT-styleä¸­æ–‡é‡Šä¹‰å’Œå‡ºç°æ—¶é—´/å°è¯ï¼Œæ–¹ä¾¿ç†è§£ä¸å®šä½ã€‚

## How to Use

1. Open the deployed app or run the local dev server.
2. Click â€œChoose subtitle fileâ€ on the left panel and upload an `.srt` file (bilingual subtitles are fine; only English words are extracted).
3. Review the stats and high-frequency list to build cards or export the data elsewhere.

## Local Development

```bash
npm install        # install dependencies
npm run dev        # start the dev server
npm run build      # create a production build
npm run preview    # preview the production build
```

The project uses React, TypeScript, Vite, Tailwind CSS, and shadcn/ui components. Main logic lives under `src/`, with the core workflow in `src/App.tsx`.

## Updating the Difficult-Word List

The file `src/data/difficultWords.ts` is auto-generated. To tweak the vocabulary bank:

1. Edit `data/wordSources.json` to adjust sources, difficulty tiers, or manual words. Each entry can either point to a manual list or describe a frequency range from the SUBTLEXus spoken corpus.
2. Run `npm run update-word-list` to regenerate `src/data/difficultWords.ts`.
3. Restart the dev server (if running) so Vite picks up the changes.

This setup lets us manage layered word sourcesâ€”foundation, intermediate, advancedâ€”without touching application code. Optional helpers:

- `data/ngsl.csv` â€” drop the New General Service List here (e.g., the 2.8k-word CSV). The update script will merge it with the top SUBTLEX words to auto-build a **common words** blacklist.
- `data/commonWords.json` â€” generated union of NGSL + SUBTLEX é«˜é¢‘è¯ï¼Œå¯ä¾›å…¶ä»–å·¥å…·å¤ç”¨ã€‚
- `data/excludedWords.json` â€” æ‰‹åŠ¨è·³è¿‡çš„å•è¯åˆ—è¡¨ï¼ˆä¾‹å¦‚å¸¸è§åŠ¨è¯æ´¾ç”Ÿï¼‰ã€‚
- ç”Ÿæˆè„šæœ¬ä¼šåŒæ­¥å†™å‡º `data/difficultWords.json`ï¼Œä¾›å‘½ä»¤è¡Œå·¥å…·æˆ–å…¶ä»–æœåŠ¡è¯»å–ã€‚
- The generator normalizesè¯å½¢ï¼ˆä½¿ç”¨ ecdict çš„ `findLemma`ï¼‰å¹¶è‡ªåŠ¨æŒ‚ä¸Š CC-CEDICT é‡Šä¹‰/éŸ³æ ‡ã€‚

### Subtitle Word Analysis CLI

Use the helper CLI to inspect any `.srt` file and export every word it uses together with counts + difficulty metadata:

```bash
npm run analyze-subtitle -- samples/subtitles/Stranger.things.S04E08.GGEZ.English-WWW.MY-SUBS.CO.srt --out analysis.csv
```

The CSV columns are `word,count,isDifficult,difficulty,translation,definition,phonetic`. Omit `--out` to print to stdout.

### Marking "Too Easy" Words

- åœ¨ UI çš„éš¾è¯åˆ—è¡¨ä¸­ç‚¹å‡» â€œå¤ªç®€å•â€ æŒ‰é’®å³å¯æ ‡è®°/å–æ¶ˆï¼Œè¯¥çŠ¶æ€ä¼šä¿å­˜åœ¨æµè§ˆå™¨ `localStorage`ã€‚
- ç‚¹å‡»åˆ—è¡¨ä¸‹æ–¹çš„ â€œä¸‹è½½ JSONâ€ æˆ– â€œå¤åˆ¶å†…å®¹â€ è·å– `too-easy` æ¸…å•ã€‚
- è¿è¡Œ `npm run apply-too-easy -- path/to/too-easy.json` å¯å°†è¿™äº›è¯åˆå¹¶è¿› `data/excludedWords.json`ï¼Œéšåå†æ‰§è¡Œ `npm run update-word-list` è®©å‰ç«¯å¿½ç•¥è¿™äº›è¯ã€‚

## Roadmap

- Export the word list to CSV/Anki.
- Allow custom difficult-word lists.
- Add accounts to store multiple parsing sessions.
